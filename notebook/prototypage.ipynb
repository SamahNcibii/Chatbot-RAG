{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a3719c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Dossier projet: c:\\Chatbot-RAG\n",
      "üìÅ Donn√©es: c:\\Chatbot-RAG\\data\\TRANS_TXT\n",
      "üîß Chargement du mod√®le d'embedding local...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55c07843a5b458a8bba4bb6646baa30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a819ff4616f545cd9056804eb48cc1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c76495a71142aaaa6b0f0bd30c9b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a74c703c5242f995faedf04d3199b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f71f8a40d94cac9add4e3f9b1c45f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f65a0da673e47d88bb9764a5b5f50a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1168cd05ca41baaf880b6d6a01fbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/463 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718bd5cfc3c44c7aad1a8ea3154cb489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c81271a07794902bcdafc7f3636046c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b283e819754f758730d009dd4028a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351fa644551b443d91d105299f4b6f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le charg√©: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'CamembertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "   Dimensions: 768\n",
      "‚úÖ Connexion PostgreSQL r√©ussie\n",
      "‚úÖ Base de donn√©es pr√™te\n",
      "üìÅ 41 fichiers .txt trouv√©s\n",
      "  ‚úì 017_00000012.txt (2318 caract√®res)\n",
      "  ‚úì 018_00000013.txt (409 caract√®res)\n",
      "  ‚úì 019_00000014.txt (687 caract√®res)\n",
      "  ‚úì 020_00000015.txt (511 caract√®res)\n",
      "  ‚úì 022_00000017.txt (1068 caract√®res)\n",
      "\n",
      "üìä 5 documents charg√©s\n",
      "\n",
      "üîß G√©n√©ration des embeddings locaux...\n",
      "  Document 1/5: 017_00000012.txt\n",
      "    ‚úì Embedding g√©n√©r√© (768 dimensions)\n",
      "  Document 2/5: 018_00000013.txt\n",
      "    ‚úì Embedding g√©n√©r√© (768 dimensions)\n",
      "  Document 3/5: 019_00000014.txt\n",
      "    ‚úì Embedding g√©n√©r√© (768 dimensions)\n",
      "  Document 4/5: 020_00000015.txt\n",
      "    ‚úì Embedding g√©n√©r√© (768 dimensions)\n",
      "  Document 5/5: 022_00000017.txt\n",
      "    ‚úì Embedding g√©n√©r√© (768 dimensions)\n",
      "\n",
      "üíæ Insertion dans la base...\n",
      "  ‚úì 017_00000012.txt (ins√©r√©)\n",
      "  ‚úì 018_00000013.txt (ins√©r√©)\n",
      "  ‚úì 019_00000014.txt (ins√©r√©)\n",
      "  ‚úì 020_00000015.txt (ins√©r√©)\n",
      "  ‚úì 022_00000017.txt (ins√©r√©)\n",
      "\n",
      "üìä R√©sultat: 5 ins√©r√©s, 0 ignor√©s\n",
      "\n",
      "==================================================\n",
      "CHATBOT RAG - EMBEDDINGS LOCAUX\n",
      "==================================================\n",
      "\n",
      "üîç √âTAT:\n",
      "  ‚Ä¢ Mod√®le embedding: ‚úÖ Charg√©\n",
      "  ‚Ä¢ PostgreSQL: ‚úÖ Connect√©\n",
      "  ‚Ä¢ Documents charg√©s: 5\n",
      "\n",
      "üìä STATISTIQUES\n",
      "--------------------\n",
      "üìÑ Documents: 5\n",
      "üîß Avec embeddings: 5\n",
      "üí¨ Requ√™tes historis√©es: 0\n",
      "\n",
      "==================================================\n",
      "MENU:\n",
      "1. Mode interactif\n",
      "2. D√©monstration\n",
      "3. Quitter\n"
     ]
    }
   ],
   "source": [
    "# prototypage.ipynb - Version avec embeddings locaux\n",
    "\n",
    "# ========== 1. IMPORTATION DES LIBRAIRIES ==========\n",
    "import os\n",
    "import sys\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer  # <-- MOD√àLE LOCAL\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========== 2. CONFIGURATION DES CHEMINS ==========\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent\n",
    "data_dir = project_root / \"data\"\n",
    "txt_dir = data_dir / \"TRANS_TXT\"\n",
    "env_file = project_root / \".env\"\n",
    "\n",
    "print(f\"üìÅ Dossier projet: {project_root}\")\n",
    "print(f\"üìÅ Donn√©es: {txt_dir}\")\n",
    "\n",
    "# ========== 3. CHARGER LE MOD√àLE D'EMBEDDING LOCAL ==========\n",
    "def load_embedding_model():\n",
    "    \"\"\"Charger un mod√®le d'embedding local\"\"\"\n",
    "    print(\"üîß Chargement du mod√®le d'embedding local...\")\n",
    "    try:\n",
    "        # Mod√®le fran√ßais l√©ger et performant\n",
    "        model = SentenceTransformer('dangvantuan/sentence-camembert-base')\n",
    "        print(f\"‚úÖ Mod√®le charg√©: {model}\")\n",
    "        print(f\"   Dimensions: {model.get_sentence_embedding_dimension()}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur chargement mod√®le: {e}\")\n",
    "        # Alternative\n",
    "        try:\n",
    "            model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            print(f\"‚úÖ Mod√®le alternatif charg√©\")\n",
    "            return model\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "embedding_model = load_embedding_model()\n",
    "\n",
    "# ========== 4. CONNEXION √Ä POSTGRESQL ==========\n",
    "def connect_postgres():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"rag_chatbot\",\n",
    "            user=\"postgres\",\n",
    "            password=\"samah\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        print(\"‚úÖ Connexion PostgreSQL r√©ussie\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "conn = connect_postgres()\n",
    "\n",
    "# ========== 5. CR√âATION/MISE √Ä JOUR DES TABLES ==========\n",
    "def setup_database():\n",
    "    if not conn:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Activer pgvector\n",
    "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "        \n",
    "        # Table documents - ajuster la dimension selon le mod√®le\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS documents (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                file_name VARCHAR(300),\n",
    "                content TEXT NOT NULL,\n",
    "                embedding vector(384),  -- 384 pour all-MiniLM-L6-v2\n",
    "                metadata JSONB,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Table chat_history\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS chat_history (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                user_query TEXT,\n",
    "                bot_response TEXT,\n",
    "                sources TEXT[],\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        print(\"‚úÖ Base de donn√©es pr√™te\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur base de donn√©es: {e}\")\n",
    "\n",
    "if conn:\n",
    "    setup_database()\n",
    "\n",
    "# ========== 6. CHARGEMENT DES FICHIERS ==========\n",
    "def load_text_files(max_files=10):\n",
    "    if not txt_dir.exists():\n",
    "        print(f\"‚ùå Dossier introuvable: {txt_dir}\")\n",
    "        return []\n",
    "    \n",
    "    txt_files = list(txt_dir.glob(\"*.txt\"))\n",
    "    if not txt_files:\n",
    "        print(\"‚ùå Aucun fichier .txt trouv√©\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üìÅ {len(txt_files)} fichiers .txt trouv√©s\")\n",
    "    \n",
    "    documents = []\n",
    "    for i, file_path in enumerate(txt_files[:max_files]):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Nettoyage\n",
    "            content = re.sub(r'\\s+', ' ', content).strip()\n",
    "            \n",
    "            if len(content) > 50:\n",
    "                documents.append({\n",
    "                    'id': i + 1,\n",
    "                    'file_name': file_path.name,\n",
    "                    'content': content,\n",
    "                    'file_path': str(file_path),\n",
    "                    'length': len(content)\n",
    "                })\n",
    "                print(f\"  ‚úì {file_path.name} ({len(content)} caract√®res)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó {file_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüìä {len(documents)} documents charg√©s\")\n",
    "    return documents\n",
    "\n",
    "documents = load_text_files(max_files=5)\n",
    "\n",
    "# ========== 7. G√âN√âRATION D'EMBEDDINGS LOCAUX ==========\n",
    "def generate_local_embedding(text: str):\n",
    "    \"\"\"G√©n√©rer un embedding avec le mod√®le local\"\"\"\n",
    "    if not embedding_model:\n",
    "        print(\"‚ùå Mod√®le d'embedding non charg√©\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Tronquer si trop long\n",
    "        if len(text) > 1000:\n",
    "            text = text[:1000]\n",
    "        \n",
    "        # G√©n√©rer l'embedding\n",
    "        embedding = embedding_model.encode(text)\n",
    "        \n",
    "        # Convertir en liste pour PostgreSQL\n",
    "        return embedding.tolist()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur g√©n√©ration embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_embeddings_to_docs(docs: List[Dict]):\n",
    "    \"\"\"Ajouter les embeddings aux documents\"\"\"\n",
    "    if not embedding_model:\n",
    "        print(\"‚ùå Impossible de g√©n√©rer les embeddings\")\n",
    "        return docs\n",
    "    \n",
    "    print(\"\\nüîß G√©n√©ration des embeddings locaux...\")\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"  Document {i+1}/{len(docs)}: {doc['file_name']}\")\n",
    "        \n",
    "        embedding = generate_local_embedding(doc['content'])\n",
    "        if embedding is not None:\n",
    "            doc['embedding'] = embedding\n",
    "            print(f\"    ‚úì Embedding g√©n√©r√© ({len(embedding)} dimensions)\")\n",
    "        else:\n",
    "            doc['embedding'] = None\n",
    "            print(f\"    ‚úó √âchec embedding\")\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Ajouter les embeddings\n",
    "if documents and embedding_model:\n",
    "    documents = add_embeddings_to_docs(documents)\n",
    "\n",
    "# ========== 8. INSERTION DANS POSTGRESQL ==========\n",
    "def insert_documents(docs: List[Dict]):\n",
    "    if not conn:\n",
    "        print(\"‚ùå Pas de connexion PostgreSQL\")\n",
    "        return 0\n",
    "    \n",
    "    inserted = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    print(\"\\nüíæ Insertion dans la base...\")\n",
    "    \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        for doc in docs:\n",
    "            if not doc.get('embedding'):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # V√©rifier si existe d√©j√†\n",
    "                cur.execute(\n",
    "                    \"SELECT id FROM documents WHERE file_name = %s\",\n",
    "                    (doc['file_name'],)\n",
    "                )\n",
    "                \n",
    "                if cur.fetchone():\n",
    "                    skipped += 1\n",
    "                    print(f\"  ‚è≠Ô∏è  {doc['file_name']} (d√©j√† pr√©sent)\")\n",
    "                else:\n",
    "                    # Ins√©rer\n",
    "                    cur.execute(\"\"\"\n",
    "                        INSERT INTO documents (file_name, content, embedding, metadata)\n",
    "                        VALUES (%s, %s, %s, %s)\n",
    "                    \"\"\", (\n",
    "                        doc['file_name'],\n",
    "                        doc['content'],\n",
    "                        doc['embedding'],\n",
    "                        json.dumps({\n",
    "                            'source': 'UBS_TXT',\n",
    "                            'length': doc['length'],\n",
    "                            'embedding_type': 'local'\n",
    "                        })\n",
    "                    ))\n",
    "                    inserted += 1\n",
    "                    print(f\"  ‚úì {doc['file_name']} (ins√©r√©)\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó {doc['file_name']}: {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur insertion: {e}\")\n",
    "        conn.rollback()\n",
    "    \n",
    "    print(f\"\\nüìä R√©sultat: {inserted} ins√©r√©s, {skipped} ignor√©s\")\n",
    "    return inserted\n",
    "\n",
    "# Ins√©rer les documents\n",
    "if documents:\n",
    "    insert_documents(documents)\n",
    "\n",
    "# ========== 9. RECHERCHE DE SIMILARIT√â ==========\n",
    "def search_similar(query: str, limit: int = 3):\n",
    "    \"\"\"Rechercher documents similaires\"\"\"\n",
    "    if not conn or not embedding_model:\n",
    "        print(\"‚ùå Connexion ou mod√®le manquant\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Embedding de la requ√™te\n",
    "        query_embedding = generate_local_embedding(query)\n",
    "        if not query_embedding:\n",
    "            return []\n",
    "        \n",
    "        cur = conn.cursor(cursor_factory=RealDictCursor)\n",
    "        \n",
    "        # Recherche\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                id,\n",
    "                file_name,\n",
    "                content,\n",
    "                embedding <=> %s as distance,\n",
    "                metadata\n",
    "            FROM documents\n",
    "            WHERE embedding IS NOT NULL\n",
    "            ORDER BY distance ASC\n",
    "            LIMIT %s\n",
    "        \"\"\", (query_embedding, limit))\n",
    "        \n",
    "        results = cur.fetchall()\n",
    "        cur.close()\n",
    "        \n",
    "        # Formater\n",
    "        formatted = []\n",
    "        for i, row in enumerate(results):\n",
    "            similarity = 1 - float(row['distance'])\n",
    "            formatted.append({\n",
    "                'rank': i + 1,\n",
    "                'file': row['file_name'],\n",
    "                'content': row['content'][:500] + \"...\" if len(row['content']) > 500 else row['content'],\n",
    "                'full_content': row['content'],\n",
    "                'similarity': round(similarity, 3)\n",
    "            })\n",
    "        \n",
    "        return formatted\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur recherche: {e}\")\n",
    "        return []\n",
    "\n",
    "# ========== 10. G√âN√âRATION DE R√âPONSE (avec Gemini ou local) ==========\n",
    "def generate_response(query: str, context_docs: List[Dict]):\n",
    "    \"\"\"G√©n√©rer une r√©ponse avec le contexte\"\"\"\n",
    "    if not context_docs:\n",
    "        return \"Je n'ai pas trouv√© d'informations pertinentes dans ma base de donn√©es.\"\n",
    "    \n",
    "    # Pr√©parer le contexte\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(context_docs):\n",
    "        context_parts.append(f\"[Document {i+1}: {doc['file']}]\\n{doc['full_content'][:800]}\")\n",
    "    \n",
    "    context_text = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # OPTION 1: Avec Gemini (si disponible)\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv(env_file)\n",
    "        api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "        \n",
    "        if api_key:\n",
    "            import google.generativeai as genai\n",
    "            genai.configure(api_key=api_key)\n",
    "            \n",
    "            prompt = f\"\"\"Tu es un assistant en sant√© num√©rique. Utilise le contexte pour r√©pondre.\n",
    "\n",
    "CONTEXTE:\n",
    "{context_text}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "R√âPONSE (en fran√ßais, bas√©e uniquement sur le contexte):\"\"\"\n",
    "            \n",
    "            model = genai.GenerativeModel('gemini-pro')\n",
    "            response = model.generate_content(prompt, generation_config={'temperature': 0.3})\n",
    "            return response.text\n",
    "    except:\n",
    "        pass  # Passe √† l'option locale\n",
    "    \n",
    "    # OPTION 2: R√©ponse locale simple (fallback)\n",
    "    prompt = f\"\"\"Question: {query}\n",
    "\n",
    "Contexte disponible:\n",
    "{context_text}\n",
    "\n",
    "En te basant sur ces documents, r√©ponds de mani√®re concise:\"\"\"\n",
    "    \n",
    "    # R√©ponse basique\n",
    "    doc_summaries = []\n",
    "    for doc in context_docs:\n",
    "        doc_summaries.append(f\"‚Ä¢ {doc['file']} (pertinence: {doc['similarity']:.1%})\")\n",
    "    \n",
    "    return f\"D'apr√®s les documents trouv√©s:\\n\" + \"\\n\".join(doc_summaries) + \\\n",
    "           f\"\\n\\nR√©ponse synth√©tique: Les documents traitent de sujets li√©s √† la requ√™te '{query}'.\"\n",
    "\n",
    "# ========== 11. PIPELINE RAG COMPLET ==========\n",
    "def rag_pipeline(query: str, top_k: int = 3):\n",
    "    \"\"\"Pipeline RAG complet\"\"\"\n",
    "    print(f\"\\nüîç Requ√™te: '{query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 1. Recherche\n",
    "    print(\"üìö Recherche de documents...\")\n",
    "    similar_docs = search_similar(query, limit=top_k)\n",
    "    \n",
    "    if not similar_docs:\n",
    "        return \"‚ùå Aucun document pertinent trouv√©.\"\n",
    "    \n",
    "    print(f\"‚úÖ {len(similar_docs)} documents trouv√©s:\")\n",
    "    for doc in similar_docs:\n",
    "        print(f\"  {doc['rank']}. {doc['file']} (score: {doc['similarity']})\")\n",
    "    \n",
    "    # 2. G√©n√©ration\n",
    "    print(\"\\nü§ñ G√©n√©ration de la r√©ponse...\")\n",
    "    response = generate_response(query, similar_docs)\n",
    "    \n",
    "    # 3. Sauvegarde historique\n",
    "    if conn:\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            sources = [doc['file'] for doc in similar_docs]\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO chat_history (user_query, bot_response, sources)\n",
    "                VALUES (%s, %s, %s)\n",
    "            \"\"\", (query, response, sources))\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            print(\"üíæ Conversation sauvegard√©e\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Erreur historique: {e}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# ========== 12. STATISTIQUES ==========\n",
    "def show_stats():\n",
    "    if not conn:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        cur.execute(\"SELECT COUNT(*) as total FROM documents\")\n",
    "        total = cur.fetchone()[0]\n",
    "        \n",
    "        cur.execute(\"SELECT COUNT(*) as with_emb FROM documents WHERE embedding IS NOT NULL\")\n",
    "        with_emb = cur.fetchone()[0]\n",
    "        \n",
    "        cur.execute(\"SELECT COUNT(*) FROM chat_history\")\n",
    "        queries = cur.fetchone()[0]\n",
    "        \n",
    "        print(\"\\nüìä STATISTIQUES\")\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"üìÑ Documents: {total}\")\n",
    "        print(f\"üîß Avec embeddings: {with_emb}\")\n",
    "        print(f\"üí¨ Requ√™tes historis√©es: {queries}\")\n",
    "        \n",
    "        cur.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur stats: {e}\")\n",
    "\n",
    "# ========== 13. D√âMONSTRATION ==========\n",
    "def run_demo():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ü§ñ D√âMONSTRATION CHATBOT RAG\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    test_queries = [\n",
    "        \"sant√© num√©rique\",\n",
    "        \"formation\",\n",
    "        \"recherche\",\n",
    "        \"universit√©\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n‚ùì Question: {query}\")\n",
    "        response = rag_pipeline(query, top_k=2)\n",
    "        print(f\"\\nüí¨ R√©ponse:\\n{response[:300]}...\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# ========== 14. INTERFACE INTERACTIVE ==========\n",
    "def interactive_mode():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üí¨ MODE INTERACTIF\")\n",
    "    print(\"Commands: stats, demo, quit\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nüë§ Vous: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"üëã Au revoir!\")\n",
    "                break\n",
    "            elif user_input.lower() == 'stats':\n",
    "                show_stats()\n",
    "                continue\n",
    "            elif user_input.lower() == 'demo':\n",
    "                run_demo()\n",
    "                continue\n",
    "            \n",
    "            response = rag_pipeline(user_input, top_k=3)\n",
    "            print(f\"\\nü§ñ Assistant: {response}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Interrompu\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur: {e}\")\n",
    "\n",
    "# ========== 15. EX√âCUTION PRINCIPALE ==========\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHATBOT RAG - EMBEDDINGS LOCAUX\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nüîç √âTAT:\")\n",
    "print(f\"  ‚Ä¢ Mod√®le embedding: {'‚úÖ Charg√©' if embedding_model else '‚ùå Absent'}\")\n",
    "print(f\"  ‚Ä¢ PostgreSQL: {'‚úÖ Connect√©' if conn else '‚ùå Absent'}\")\n",
    "print(f\"  ‚Ä¢ Documents charg√©s: {len(documents)}\")\n",
    "\n",
    "show_stats()\n",
    "\n",
    "# Menu\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MENU:\")\n",
    "print(\"1. Mode interactif\")\n",
    "print(\"2. D√©monstration\")\n",
    "print(\"3. Quitter\")\n",
    "\n",
    "choice = input(\"\\nüëâ Choix (1-3): \").strip()\n",
    "\n",
    "if choice == \"1\":\n",
    "    interactive_mode()\n",
    "elif choice == \"2\":\n",
    "    run_demo()\n",
    "elif choice == \"3\":\n",
    "    print(\"üëã Au revoir!\")\n",
    "else:\n",
    "    print(\"‚ùå Choix invalide\")\n",
    "\n",
    "if conn:\n",
    "    conn.close()\n",
    "    print(\"\\n‚úÖ Connexions ferm√©es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291380af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans une cellule du notebook\n",
    "def update_table_dimension():\n",
    "    \"\"\"Mettre √† jour la table pour les embeddings de 768 dimensions\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"rag_chatbot\",\n",
    "            user=\"postgres\",\n",
    "            password=\"samah\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Supprimer et recr√©er la table avec la bonne dimension\n",
    "        cur.execute(\"DROP TABLE IF EXISTS documents CASCADE\")\n",
    "        \n",
    "        # Recr√©er avec 768 dimensions\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE documents (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                file_name VARCHAR(300),\n",
    "                content TEXT NOT NULL,\n",
    "                embedding vector(768),  -- 768 dimensions pour Camembert\n",
    "                metadata JSONB,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        print(\"‚úÖ Table mise √† jour pour 768 dimensions\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "\n",
    "# Ex√©cutez cette fonction\n",
    "update_table_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test direct du chatbot\n",
    "def test_chatbot():\n",
    "    print(\"ü§ñ TEST DU CHATBOT RAG\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Questions de test\n",
    "    test_questions = [\n",
    "        \"Qu'est-ce que la sant√© num√©rique ?\",\n",
    "        \"Parle-moi des formations\",\n",
    "        \"Quelles recherches sont men√©es ?\",\n",
    "        \"Qui sont les enseignants ?\"\n",
    "    ]\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\n‚ùì Question: {question}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Recherche\n",
    "        results = search_similar(question, limit=2)\n",
    "        if not results:\n",
    "            print(\"‚ùå Aucun r√©sultat trouv√©\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"üìö Documents trouv√©s ({len(results)}):\")\n",
    "        for doc in results:\n",
    "            print(f\"  ‚Ä¢ {doc['file']} (score: {doc['similarity']:.3f})\")\n",
    "        \n",
    "        # G√©n√©ration r√©ponse\n",
    "        response = generate_response(question, results)\n",
    "        print(f\"\\nüí¨ R√©ponse:\\n{response[:500]}...\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "# Ex√©cutez le test\n",
    "test_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_search(query, limit=3, similarity_threshold=0.3):\n",
    "    \"\"\"Recherche intelligente avec seuil de similarit√©\"\"\"\n",
    "    results = search_similar(query, limit=limit*2)  # Chercher plus de r√©sultats\n",
    "    \n",
    "    # Filtrer par seuil de similarit√©\n",
    "    filtered = [doc for doc in results if doc['similarity'] >= similarity_threshold]\n",
    "    \n",
    "    # Limiter au nombre demand√©\n",
    "    return filtered[:limit]\n",
    "\n",
    "# Test\n",
    "print(\"üîç Recherche intelligente\")\n",
    "query = \"sant√©\"\n",
    "results = smart_search(query, limit=2, similarity_threshold=0.2)\n",
    "\n",
    "if results:\n",
    "    print(f\"‚úÖ {len(results)} r√©sultats pertinents:\")\n",
    "    for r in results:\n",
    "        print(f\"  ‚Ä¢ {r['file']} (score: {r['similarity']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a6657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_documents():\n",
    "    \"\"\"Analyser le contenu des documents\"\"\"\n",
    "    print(\"üìä ANALYSE DES DOCUMENTS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    for doc in documents:\n",
    "        print(f\"\\nüìÑ {doc['file_name']}:\")\n",
    "        print(f\"   Caract√®res: {doc['length']}\")\n",
    "        \n",
    "        # Afficher les 200 premiers caract√®res\n",
    "        preview = doc['content'][:200].replace('\\n', ' ')\n",
    "        print(f\"   Extrait: {preview}...\")\n",
    "        \n",
    "        # Mots cl√©s\n",
    "        words = doc['content'].lower().split()[:10]\n",
    "        print(f\"   Mots: {', '.join(words)}...\")\n",
    "\n",
    "analyze_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc08421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chat_interface():\n",
    "    \"\"\"Interface chat simple\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üí¨ CHATBOT RAG - UBS SANT√â NUM√âRIQUE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Tapez votre question ou 'quit' pour quitter\")\n",
    "    print(\"Tapez 'stats' pour les statistiques\")\n",
    "    print(\"Tapez 'docs' pour voir les documents\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nüë§ Vous: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"üëã Au revoir!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'stats':\n",
    "            show_stats()\n",
    "            continue\n",
    "        elif user_input.lower() == 'docs':\n",
    "            print(\"\\nüìÅ Documents dans la base:\")\n",
    "            try:\n",
    "                conn = connect_postgres()\n",
    "                cur = conn.cursor()\n",
    "                cur.execute(\"SELECT file_name, LENGTH(content) as length FROM documents\")\n",
    "                for row in cur.fetchall():\n",
    "                    print(f\"  ‚Ä¢ {row[0]} ({row[1]} caract√®res)\")\n",
    "                cur.close()\n",
    "                conn.close()\n",
    "            except:\n",
    "                print(\"  Impossible d'acc√©der √† la base\")\n",
    "            continue\n",
    "        \n",
    "        # Traiter la question\n",
    "        print(\"üîç Recherche en cours...\")\n",
    "        \n",
    "        # Recherche intelligente\n",
    "        results = smart_search(user_input, limit=3, similarity_threshold=0.2)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"ü§ñ Assistant: Je n'ai pas trouv√© d'informations pertinentes sur ce sujet.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"üìö {len(results)} documents pertinents trouv√©s\")\n",
    "        \n",
    "        # G√©n√©rer la r√©ponse\n",
    "        response = generate_response(user_input, results)\n",
    "        \n",
    "        print(f\"\\nü§ñ Assistant: {response}\")\n",
    "        \n",
    "        # Afficher les sources\n",
    "        print(\"\\nüìé Sources utilis√©es:\")\n",
    "        for doc in results:\n",
    "            print(f\"  ‚Ä¢ {doc['file']} (pertinence: {doc['similarity']:.1%})\")\n",
    "\n",
    "# Lancez l'interface\n",
    "simple_chat_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aecbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour d√©marrer directement l'interface chat\n",
    "simple_chat_interface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
